---
title: "Week 10: When Static Analysis Meets Large Language Models: A Neuro-Symbolic Approach"
speakers: Dr. Chengpeng Wang, Purdue University
date: 2024-10-29
layout: post
---

ðŸ’¬ **Dr.Chengpeng Wang:** Static analysis plays an essential role in program optimization, debugging, and security auditing. However, reliance on compilation processes and limited customization hinder its adoption in real-world scenarios. In this talk, I will introduce a neuro-symbolic paradigm of static analysis that facilitates customizable and compilation-free analysis. Specifically, I will present our works, LLMDFA and LLMSAN, which were recently accepted in NeurIPS'24 and Findings of EMNLP'24, respectively. Unlike traditional static analyzers, our approach allows users to customize the analysis by providing prompts with few-shot examples. Using these prompts, LLMs directly interpret program semantics upon source code without the need for compilation. Notably, LLMDFA and LLMSAN leverage several symbolic analysis tools, such as program parsers and SMT solvers, to reduce LLMS hallucinations during reasoning, thereby yielding high-quality analysis results. I will also compare our methods with the latest reasoning model, OpenAI o1, and share insights into their connections. Finally, I will conclude the talk by discussing several open problems and future directions for LLM-driven static analysis.